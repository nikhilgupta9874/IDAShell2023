import findspark

findspark.init()

findspark.find()

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("tempApp").getOrCreate()

sc = spark.sparkContext

rdd = sc.parallelize([1,23,4,5])
rdd.collect()

resultrdd = rdd.map(lambda x : x * 2)
resultrdd.collect()

rdd1 = sc.parallelize([1,2,3,4,5])

resultrdd1=rdd1.flatMap(lambda x:(x,x*2))
resultrdd1.collect()

rdd3=sc.parallelize([1,2,3,4,56,7])

resultrdd3=rdd3.filter(lambda x : x%2==0)
resultrdd3.collect()

rdd4=sc.parallelize([(1,2),(3,4),(1,6),(2,3)])
resrdd4 = rdd4.reduceByKey(lambda x,y : x+y)#aggregating based on same key
resrdd4.collect()

import findspark

pyspark.find()


findspark.init()

findspark.find()

rdd5=sc.parallelize([(1,2),(3,4),(1,6),(2,3)])
resrdd5 = rdd5.groupByKey()#aggregating based on same key
resrdd5.collect()

for key,values in resrdd5.collect():
    print(f"Key: {key}, Values: {list(values)}")

word_list=["this", "is", "a", "sample", "text", "for", "word", "count", "example", "word", "count"]

rdd6 = sc.parallelize(word_list)

# wordcount = rdd6.flatMap(lambda x: (x, word_list.count(x)))
samp = []
for word in word_list:
    samp.append((word, 1))
rdd7 = sc.parallelize(samp)
ans = rdd7.groupByKey().mapValues(len)
ans1 = rdd7.reduceByKey(lambda a,b: a + b)
# wordcount.collect()
ans1.collect()


purchaserdd=sc.textFile('/home/labuser/Downloads/Pandas_datasets/purchases.csv')

purchaserdd.collect()

purchasedf=spark.read.csv('/home/labuser/Downloads/Pandas_datasets/purchases.csv')

purchasedf.show()

purchasedf_01=spark.read.option("inferSchema",True).option("header",True).csv('/home/labuser/Downloads/Pandas_datasets/purchases.csv')
purchasedf_01.show()
#inferSchema not preferred because it reads through the whole file and takes a lot of time

purchasedf_01.printSchema()

purchasedf.printSchema()

from pyspark.sql.types import StructType, StructField, IntegerType, StringType
udfschema = StructType([StructField("Rank", IntegerType(), True), StructField("Title", StringType(), True), StructField("Genre", StringType(), True)])
imdb=spark.read.schema(udfschema).option("header",True).csv('/home/labuser/Downloads/Pandas_datasets/IMDB-Movie-Data.csv')

imdb.show()

imdb.printSchema()

imdbdf_01=spark.read.option("inferSchema",True).option("header",True).csv('/home/labuser/Downloads/Pandas_datasets/IMDB-Movie-Data.csv')

imdbdf_01.show()

from pyspark.sql.functions import *
imdbdf_01 = imdbdf_01.withColumn("rev_new", col("Revenue (Millions)")*100)

imdbdf_01.show()

imdbdf_01 = imdbdf_01.withColumn("rev_new", col("Revenue (Millions)")*100).withColumn("batch", lit("Batch 03")) ##adding a new column batch with every value as batch 03

import time

from pyspark.sql.functions import *
from datetime import datetime

now = datetime.now()

current_time = now.strftime("%H:%M:%S")

imdbdf_01 = imdbdf_01.withColumn("updated_time", lit(now))

imdbdf_01.show()

imdbdf_01.rdd.getNumPartitions()

newdf = imdbdf_01.repartition(8)

newdf.rdd.getNumPartitions()

newdf.write.csv('/home/labuser/Desktop/out/out.csv')

spark_ui_url=f'{spark._jsc.sc().uiWebUrl().get()}/'

print(spark_ui_url)

imdbdf_01.createOrReplaceTempView("purchase")

result = spark.sql("select * from purchase")

result.show()

from pyspark.sql.functions import col
testdf=imdbdf_01.sort(col("Title")) # sorting

testdf.show()

df = imdbdf_01.select('Title', 'Rank')

df

df.show()

NEWdf = imdbdf_01.selectExpr('CAST(updated_time AS DATE) AS test', 'Title') # casting date column

NEWdf.show()

alias = imdbdf_01.select(col("Title").alias("MovieName"))
alias.show()

aliasRepeat = alias.withColumn("Title", col("MovieName"))
aliasRepeat.show()

dropd = imdbdf_01.dropDuplicates(["Director"])# removing duplicates

imdbdf_01.count()

dropd.count()

dropd1 = imdbdf_01.dropDuplicates(["Director", "Title"])
dropd1.count()

ratingVal = imdbdf_01.withColumn("RatingValue", when((col("Rating")>=8) & (col("Rating")<=10), "Best").when((col("Rating")>=5) & (col("Rating")<8), "Good").when((col("Rating")>=0) & (col("Rating")<5), "Average").otherwise("NA"))

ratingVal.show()

ratingVal.select(col("RatingValue")).show()

func = imdbdf_01.withColumn("Func", (concat(col("Title"), "_shell")))

func.select(col("Func")).show()

def concat_shell(column):
    return column + "_shell"

concat_shell("test")

funcSpark = imdbdf_01.withColumn("new_Col", concat_shell(col("Title")))

funcSpark.show()

udf_a = udf(concat_shell, StringType()) # registering python function in pyspark

from pyspark.sql.functions import *

funcSparkNew = imdbdf_01.withColumn("new_Col", udf_a(col("Title")))

funcSparkNew.show()

cachedf = spark.read.option("inferSchema",True).option("header",True).csv('/home/labuser/Downloads/Pandas_datasets/IMDB-Movie-Data.csv')

cachedf.printSchema()

chachenewDf = cachedf.cache() #caching and storing in new df

chachenewDf.show()

